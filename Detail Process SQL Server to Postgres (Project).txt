Scope
1.Whats the problem?
2.What does success look like?

e-commerce company running business on SQL server for 5 years, but:
-licensing cost are expensive
-reporting queries getting slower
-they want to migrate to the cloud (AWS + PostgreSQL)

Solution
-all 4 tables migrate into PostgreSQL
-no data loss is recorded
-all data quality checks pass

Why doing this projects?
-migrate databases (with no data loss)
-validate migrations (with code+ + visuals)
-build production-grade data pipelines

Design
-What tables are we migrating?
-What does good data quality look like?
-What does the pipeline looklike?

What tables are we migrating from SQL to PostgreSQL?
-4 tables
1.Customers
2.Suppliers
3.Products
4.Categories

Pre-Process Logs(What tables are we migrating)
1.git clone https://github.com/sdw-online/sql-server-to-postgres-migration.git (in bash)(clone repo)
2.cd sql-server-to-postgres-migration (store it in local)
3.code . (open in vsc)
4.install all module (pip install in cmd)
5.debugging code of 1 million row table generator (vsc)
6.create database in SQL server(TransactionDB_UAT)
6.code running and start generating table(Categories,Customers,Products,Suplliers) into database(TransactionDB_UAT) in SQL server
7.see what fields inside 4 tables in database
8.counting total rows from table in database (SQL Server)(we need to preserve all of it) to transfer it to PostgreSQL

Pre-Process Logs(What does data quality look like)
Before migration
1.Total rows confirmed (1,055,008)
2.No NULLs in required field (flag to report)
3.No orphaned foreign keys (flag to report)
4.No negatives value found (flag to report)
5.No futures date used (flag to report)
After migration
1.row count is equal (1,055,008)
2.data types match in both system (datetime=timestamp, numeric=decimal, etc)
3.no primary key is duplicated
4.tables should be able to talk to each other correctly (all relationship must be preserved)

Pre-Process logs(What does the migration pipeline looklike)
1.Audit data (before migration)
2.Extract data from SQL Server
3.Transform data
4.Load data into PostgreSQL
5.Validate data

Process logs(anaconda prompt, gitbash, vscode)
1.creating environment > D: > cd conda_envs > cd sql-to-postgres> (base) D:\conda_envs\sql-to-postgres>conda create -n sqlserver-to-postgres python=3.11 -y (Some libraries (especially database connectors or older packages) may not yet fully support 3.12)
2.conda activate sqlserver-to-postgres
3.pip install pandas (create dataframes with data) 
4.pip install pyodbc (talk to SQL server with python)
5.pip install psycopg2-binary (talk to PostgreSQL with python)
6.pip install python-dotenv (secure our sensitive credentials and load it into session)
7.code (entering vscode)
8.go to GitHub.com > new repository > repo name = "sql-server-to-postgres" > description = "Migrating enterprise data from SQL Server to PostgreSQL" > create repo
9.open git bash > cd d:/github > git clone https://github.com/Opalfdm/sql-server-to-postgres-migration.git > cd sql-server-to-postgres-migration
10.open vscode > file > open folder > d:/github > select folder sql-server-to-postgres-migration 
11.new file(vsc) > git status > create README.md > git add README.md > save file
12.git commit -m "Created README.md file" -m "I have created a README.md file for this project" (save file first in vsc)(put it to local repo)
13.git push origin main (put it from local repo to remote repo)
14.wrote pseudo code in README.md(VSCode) 
15.new file(vsc) > .env (where we going to save all sensitive credentials from unauthorized/malicious users accessing it)(dont commit .env file into remote repo)
16.new file(vsc) > .gitignore (hide any file included from the remote repo)(type .env inside here(.gitignore))
17..env file > put ur SQL Server and PostgreSQL related info there
18.create database in PostgreSQL > name transaction_uat
19.new file(vsc) > run_migration_uat.ipynb > select kernel(env) > sqlserver-to-postgres (Python 3.11.14)
20.load credentials (import os, pandas, pyodbc, psycopg2, execute_values from psycopg2.extras, load_dotenv from dotenv)
21.connect to SQL Server
22.connect to PostgreSQL
23.defines the table to migrate (order them by dependency[tables that not depend on any table are first and tables that depend on other table can be last][PostgreSQL might reject tables with dependency to other table][foreign key constraint would fail][referential integrity]
24.pre-migration checks (counting all rows that need to be migrated from SQL Server to PostgreSQL)(DO NOT INPUT SQL QUERIES WITH F-STRINGS IN PRODUCTION BECAUSE SQL INJECTION ATTACKS)
25.pre-migration checks (NULL count[CustomerName], NEGATIVE PRODUCT PRICES, NEGATIVE STOCK QUANTITIES, ORPHAN FOREIGN KEYS, FUTURES DATE CHECKS)
26.get table schema (COLUMN_NAME,  DATA_TYPE, CHARACTER_MAXIMUM_LENGTH,  IS_NULLABLE)
27.define data type mappings (SQLServer -> PostgreSQL data type equivalent of each other)
28.create tables in PostgreSQL(dont forget to use pg_conn.rollback() so table/database is in the same state before the code running if there's an error)
29.test migration with one table
30.migrate remaining tables
31.run post-migration validation in PostgreSQL (row_count, column_name, data_type, is_nullable, ordinal_position, primary key, orphaned foreign key)(do side by side comparison)(do it in Postgres and SQL Server with query)
32.pip install matplotlib (for visualization)
33.visualization of migration validation chart (comparison of total rows from before(SQL Server) to after(Postgres) migration) 
34.step 19-33 happen in run_migration_uat.ipynb (vscode)

	                
	               


run_migration_uat.ipynb

1.Load credentials

import os
import pandas as pd
import pyodbc
import psycopg2
from psycopg2.extras import execute_values
from dotenv import load_dotenv

load_dotenv() (allow us to load our credentials from .env file into this session and safer than hard coding sensitive credentials)

sql_host = os.getenv("SQL_SERVER_HOST")
sql_db = os.getenv("SQL_SERVER_DB")

print(f"SQL_SERVER_HOST: {sql_host}")
print(f"SQL_SERVER_DB: {sql_db}")

pg_host = os.getenv("POSTGRES_HOST")
pg_port = os.getenv("POSTGRES_PORT")
pg_db = os.getenv("POSTGRES_DB")
pg_user = os.getenv("POSTGRES_USER")
pg_password = os.getenv("POSTGRES_PASSWORD")

print(f"POSTGRES_HOST: {pg_host}")
print(f"POSTGRES_PORT: {pg_port}")
print(f"POSTGRES_DB: {pg_db}")
print(f"POSTGRES_USER: {pg_user}")
print(f"POSTGRES_PASSWORD: {pg_password}")

2.Connect to SQL Server

print("Connecting to SQL Server")
print(f"  Server: {sql_host}")
print(f"  Database: {sql_db}")

try:
    sql_conn_string = (
        f"DRIVER={{ODBC Driver 17 for SQL Server}};"
        f"SERVER={sql_host};"
        f"DATABASE={sql_db};"
        f"Trusted_Connection=yes;"
    )
    
    sql_conn = pyodbc.connect(sql_conn_string)
    sql_cursor = sql_conn.cursor()
    print(f"[SUCCESS] -> Connection to SQL Server now live! ")
    
except Exception as e:
    print(f"SQL Server connection failed: {e}")
    print(""" How to troubleshoot
          > 1. Check server name in .env file is correct
          > 2. Verify SQL Server is running
          > 3. Check Windows Authentication is enabled
            ...
 """)       

3.Connect to PostgreSQL

print("Connecting to PostgreSQL...")
print(f"  Server: {pg_host}")
print(f"  Database: {pg_db}")

try:
    pg_conn = psycopg2.connect(
        host= pg_host,
        port= pg_port,
        database= pg_db,
        user= pg_user,
        password= pg_password
        
    )
    
    pg_cursor= pg_conn.cursor()
    pg_cursor.execute("SELECT version();")
    
    pg_version= pg_cursor.fetchone()[0]
    
    print("Connected to PostgreSQL")
    print(f"  Version: {pg_version[:50]}...\n")
         
except psycopg2.OperationalError as e:
    print(f" PostgreSQL connection failed: {e}")
    print(""" How to troubleshoot
            > 1. Check if Postgres is running
            > 2. Verify username + password
            > 3. Check database existence
            
            ...
""")
    
except Exception as e:
    print(f"Unexpected Error: {e}")
    raise

4.Defines table to migrate

### Migration order

- Categories (no dependencies)
- Suppliers (no dependencies)
- Customers (no dependencies)
- Products (depends on Categories and Suppliers)

tables_to_migrate = ['Categories', 'Suppliers', 'Customers', 'Products']
print(tables_to_migrate)

print("Table to migrate:")
for i, table in enumerate(tables_to_migrate, 1):
    print(f" {i}. {table}")
    
total_no_tbls = len(tables_to_migrate)
print(f"\nTotal no of tables to migrate: {total_no_tbls}")

5.Run pre-migration checks

print("=" * 50)
print(">>> Check 1: ROW COUNTS")
print("=" * 50)

baseline_counts = {}

try:
    for table in tables_to_migrate:
        row_count_query = f"SELECT COUNT(*) as total_rows FROM {table}"	#dont use f-string in production
        sql_cursor.execute(row_count_query)
        count = sql_cursor.fetchone()[0]
        
        baseline_counts[table] = count
        print(f"{table:15} {count:>12} rows")
        
    total_rows = sum(baseline_counts.values())
    print(f"{'-' * 30}")
    print(f"{'TOTAL':15} {total_rows:>12,} rows")
    print("\n Baseline captured!")
    
except Exception as e:
    print(f"Failed to get baseline counts: {e}")
    raise

quality_issues = []
try:
    print("\nCHECK 2: NULL CHECKS (CustomerName)")
    sql_cursor.execute("""SELECT COUNT(*) as null_count 
                            FROM Customers 
                            WHERE CustomerName IS NULL""")
    null_names = sql_cursor.fetchone()[0]
    if null_names > 0:
        quality_issues.append(f"    - {null_names:,} customers with NULL names")
    #print(quality_issues)
    
    print("\nCHECK 3: INVALID EMAIL FORMATS")
    sql_cursor.execute("""SELECT COUNT(*) as invalid_email_count 
                            FROM Customers 
                            WHERE Email LIKE '%@invalid'""")
    invalid_emails = sql_cursor.fetchone()[0]
    if invalid_emails > 0:
        quality_issues.append(f"    - {invalid_emails:,} customers with invalid email formats")
    #print(quality_issues)
    
    print("\nCHECK 4: NEGATIVE PRODUCT PRICES")
    sql_cursor.execute("""SELECT COUNT(*) as negative_product_prices_count 
                            FROM Products 
                            WHERE UnitPrice < 0""")
    negative_price = sql_cursor.fetchone()[0]
    if negative_price > 0:
        quality_issues.append(f"    - {negative_price:,} products with negative prices")
    #print(quality_issues)
    
    print("\nCHECK 5: NEGATIVE STOCK QUANTITIES")
    sql_cursor.execute("""SELECT COUNT(*) as negative_stock_quantities_count 
                            FROM Products 
                            WHERE StockQuantity < 0""")
    negative_stock = sql_cursor.fetchone()[0]
    if negative_stock > 0:
        quality_issues.append(f"    - {negative_stock:,} products with negative stock quantities")
    #print(quality_issues)
    
    print("\nCHECK 6: ORPHAN FOREIGN KEYS")
    sql_cursor.execute("""SELECT COUNT(*) as orphaned_records
                            FROM Products prod
                            WHERE NOT EXISTS (SELECT 1
		                    FROM Suppliers sup
		                    WHERE sup.SupplierID = prod.SupplierID)""")
    orphaned_fkeys = sql_cursor.fetchone()[0]
    if orphaned_fkeys > 0:
        quality_issues.append(f"    - {orphaned_fkeys:,} products with orphan foreign keys")
    #print(quality_issues)
    
    print("\nCHECK 7: FUTURES DATE CHECK")
    sql_cursor.execute("""SELECT COUNT(*) as futures_date_count
                            FROM Customers
                            WHERE CreatedDate > GETDATE()""")
    futures_date = sql_cursor.fetchone()[0]
    if futures_date > 0:
        quality_issues.append(f"    - {futures_date:,} customers creations data with futures date")
    #print(quality_issues)
    
    if quality_issues:
        print("\nData quality issues found (will migrate as-is)")
        for issue in quality_issues:
            print(issue)
    else:
        print("\nNo data quality issues identified")
    
except Exception as e:
    print(f"ERROR ===> Unexpected Issue: {e}")
    raise

6.Get table schema

print("=" * 65)
print("ANALZYE TABLE SCHEMA")
print("=" * 65)

table_schema = {}

try:
    for table in tables_to_migrate:
        schema_query = f"""
            SELECT	COLUMN_NAME, 
	                DATA_TYPE, 
	                CHARACTER_MAXIMUM_LENGTH,
	                IS_NULLABLE
            FROM 
                    INFORMATION_SCHEMA.COLUMNS
            WHERE 
                    table_name = '{table}'
            ORDER BY 
                    ORDINAL_POSITION"""
            
        schema_df = pd.read_sql(schema_query, sql_conn)
        print(f"\n{table}\n")
        print("-" * 10)
        print(schema_df)
        table_schema[table] = schema_df
        print("\n\n\n")
            
except Exception as e:
    pass

7.Define data type mappings

print("=" * 65)
print("DATA TYPE MAPPING")
print("=" * 65)

type_mapping = {
    'int': 'INTEGER',
    'bigint': 'BIGINT',
    'smallint': 'SMALLINT',
    'tinyint': 'SMALLINT',
    'bit': 'BOOLEAN',
    'decimal': 'NUMERIC',
    'numeric': 'NUMERIC',
    'money': 'NUMERIC(19,4)',
    'smallmoney': 'NUMERIC(10,4)',
    'float': 'DOUBLE PRECISION',
    'real': 'REAL',
    'datetime': 'TIMESTAMP',
    'datetime2': 'TIMESTAMP',
    'smalldatetime': 'TIMESTAMP',
    'date': 'DATE',
    'time': 'TIME',
    'char': 'CHAR',
    'varchar': 'VARCHAR',
    'nchar': 'CHAR',
    'nvarchar': 'TEXT',
    'text': 'TEXT',
    'ntext': 'TEXT'
    }

print("SQL Server to PostgreSQL type mapping")
print()

for sql_type, pg_type in list(type_mapping.items()):
    print(f"    {sql_type:17} ->    {pg_type}")

8.Create tables in PostgreSQL

print("=" * 65)
print("CREATE TABLES IN POSTGRES")
print("=" * 65)

try:
    for table in tables_to_migrate:
        
        schema = table_schema[table]
        
        pg_table = table.lower()
        
        pg_cursor.execute(f"DROP TABLE IF EXISTS {pg_table} CASCADE")
        
        column_definitions = []
        
        for idx, row in schema.iterrows():
            col_name = row['COLUMN_NAME'].lower()
            sql_type = row['DATA_TYPE']
            
            base_type = sql_type.lower()
            pg_type = type_mapping.get(base_type, 'TEXT')
            
            condition_1 = idx == 0                      # Must be first column in table
            condition_2 = col_name.endswith('id')       # Must end with ID
            condition_3 = 'int' in sql_type.lower()     # Must be INT data type
            
            if condition_1 and condition_2 and condition_3:
                column_definitions.append(f"{col_name} SERIAL PRIMARY KEY")
            else:
                column_definitions.append(f"{col_name} {pg_type}")
                
        column_string = ",\n        ".join(column_definitions)
        create_query = f"""
        CREATE TABLE {pg_table} (
        {column_string}
        )
        """
    
        pg_cursor.execute(create_query)
        pg_conn.commit()
    
    print("\n + " + "=" * 55)
    print("[SUCCESS] -> All tables created successfully!")

except psycopg2.Error as e:
    print(f"Postgres experienced an error while creating a table: {e}") 
    pg_conn.rollback()
    raise
    
except Exception as e:
    print(f"Unexpected Issue: {e}") 
    
9.Test migration with one table

print("=" * 65)
print("TESTING MIGRATION (SINGLE TABLE)")
print("=" * 65)

test_table = 'Customers'
table = test_table
pg_table = test_table.lower()

try:
    print("1. Read from SQL Server... ")
    extract_query = f"SELECT * FROM {table}"
    test_df = pd.read_sql(extract_query, sql_conn)
    
    print(f"        Read {len(test_df)} rows")
    
    print("2. Transforming data types... ")
    
    if 'IsActive' in test_df.columns:
        test_df['IsActive'] = test_df['IsActive'].astype('bool')
        print("[SUCCESS] -> Converted IsActive: BIT -> BOOLEAN")
        
    print("3. Prepare the data for loading")
    data_tuples = [tuple(row) for row in test_df.to_numpy()]
    columns = [col.lower() for col in test_df.columns]
    column_string = ', '.join(columns)
    placeholders = ', '.join(['%s'] * len(columns))
    insert_query =  f"""
        INSERT INTO {pg_table} ({column_string})
        VALUES %s 
    """
    print(f"        Prepared {len(data_tuples):,} rows")
    
    print("4. Insert data into PostgreSQL...")
    execute_values(pg_cursor, insert_query, data_tuples, page_size=1000)
    pg_conn.commit()
    print(f"Loaded {len(data_tuples):,} rows")
    
    print("5. Verifying...")
    pg_cursor.execute(f"SELECT COUNT(*) AS total_rows FROM {pg_table}")
    pg_count = pg_cursor.fetchone()[0]
    sql_count = baseline_counts[test_table]
    
    if pg_count == sql_count:
        print(f"[SUCCESS] --> Verification passed:  {pg_count:,} == {sql_count:,}")
    else:
        print(f"[FAILED] --> Count mismatch: {pg_count:,} != {sql_count:,}")
        
    print(f"\n {test_table} migration test successfully completed!")

except Exception as e:
    pg_conn.rollback()
    raise
    
10.Migrate remaining tables

print("=" * 65)
print("MIGRATE REMAINING TABLES")
print("=" * 65)

remaining_tables = [t for t in tables_to_migrate if t != 'Customers']

for table in remaining_tables:
    pg_table = table.lower()
    
    print(f"Migrating {table} ---> {pg_table}")
    
    try:
        print("1. Read from SQL Server... ")
        extract_query = f"SELECT * FROM {table}"
        sql_df = pd.read_sql(extract_query, sql_conn)
        print(f"    Read {len(sql_df):,} rows\n\n")
        
        print("2. Preparing data... ")
        data_tuples = [tuple(row) for row in sql_df.to_numpy()]
        columns = [col.lower() for col in sql_df.columns]
        column_string = ', '.join(columns)
        insert_query =  f"""
                INSERT INTO {pg_table} ({column_string})
                VALUES %s 
                """
        print(f"        Prepared {len(data_tuples):,} rows\n\n")
        
        print("3. Processing bulk load... ")
        execute_values(pg_cursor, insert_query, data_tuples, page_size=1000)
        pg_conn.commit()
        print(f"[SUCCESS] -> Loaded {len(data_tuples):,} rows\n\n")
        
        print("4. Verifying...")
        pg_cursor.execute(f"SELECT COUNT(*) AS total_rows FROM {pg_table}")
        pg_count = pg_cursor.fetchone()[0]
        sql_count = baseline_counts[table]
    
        if pg_count == sql_count:
            print(f"[SUCCESS] --> Verification passed:  {pg_count:,} == {sql_count:,}")
        else:
            print(f"[FAILED] --> Count mismatch: {pg_count:,} != {sql_count:,}")
        
            print(f"\n {table} migration successfully completed!")

    except Exception as e:
        print(f"Failed to migrate '{table}: {e}")
        pg_conn.rollback()
        raise

11.Run post-migration validation (visualization)

## -- 1. Row counts checks in Postgres

### SUCCESS: MATCHES SQL SERVER

## -- 2. Data type checks

### SUCCESS: MATCHES SQL SERVER

## -- 3. Primary keys checks (duplicates)

### SUCCESS: MATCHES SQL SERVER

## -- 4. Referential integrity checks (duplicates)

### SUCCESS: MATCHES SQL SERVER

import matplotlib.pyplot as plt

%matplotlib inline
plt.rcParams['figure.facecolor'] = 'white'

fig, ax = plt.subplots(figsize=(12, 6))

table_names = tables_to_migrate
sql_counts = [baseline_counts[t] for t in tables_to_migrate]

pg_counts = []
for table in tables_to_migrate:
    pg_cursor.execute(f"SELECT COUNT(*) FROM {table.lower()}")
    pg_counts.append(pg_cursor.fetchone()[0])
    
pg_counts

x = range(len(table_names))
width = 0.35

bars1 = ax.bar([i - width/2 for i in x],
              sql_counts,
              width,
              label="SQL Server",
              color="#05464BFF",
              alpha=0.8)

bars2 = ax.bar([i + width/2 for i in x],
              pg_counts,
              width,
              label="Postgres",
              color="#08C599",
              alpha=0.8)

ax.set_xlabel('Table', fontweight='bold', fontsize=12)

ax.set_title('Migration Validation: SQL Server to Postgres',  fontweight='bold', fontsize=14)

ax.set_xticks(x)
ax.set_xticklabels(table_names)

ax.get_yaxis().set_visible(False)
ax.spines['left'].set_visible(False)
ax.spines['right'].set_visible(False)
ax.spines['top'].set_visible(False)

ax.set_yscale('log')
ax.legend()

for bars in [bars1, bars2]:
    for bar in bars:
        
        height = bar.get_height()
        label = f"{int(height/1000)}K" if height >=1000 else f'{int(height)}'
        
        ax.text(bar.get_x() + bar.get_width()/2,
                height * 1.1,
                label,
                ha='center',
                va='bottom',
                fontsize=9,
                fontweight='bold')
        
plt.tight_layout()
plt.show()        